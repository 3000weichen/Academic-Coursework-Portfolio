# Assignment 1 â€“ Exploration & Dynamic Programming (Group Project)

This project covers **two core RL fundamentals**:

1ï¸âƒ£ Explorationâ€“Exploitation trade-off in **multi-armed bandits**  
2ï¸âƒ£ **Dynamic Programming** (Policy Iteration & Value Iteration) in MDPs

---

## ğŸ§ª Part 1 â€” Exploration (Bandits)
:contentReference[oaicite:0]{index=0}

We compared three action-selection strategies:

| Method | Strength | Weakness |
|--------|----------|---------|
| Îµ-greedy | Simple, converges well with moderate Îµ | Too small or too large Îµ harms performance |
| Optimistic Initialization | Forces early exploration | Over-optimism slows learning |
| UCB | Targets uncertain actions efficiently | Slower start with high exploration constant |

**Key finding**  
Moderate exploration (e.g., ğœ– = 0.05â€“0.08) achieves the best balance of learning speed and final reward.

---

## ğŸ§® Part 2 â€” Dynamic Programming (Windy GridWorld)
:contentReference[oaicite:1]{index=1}

Implemented three agents:

| Agent | Result | Efficiency |
|-------|--------|------------|
| Policy Iteration | Optimal policy | Slowest |
| Îµ-greedy Policy Iteration | Near-optimal | Fast but slightly lower expected reward |
| Value Iteration | Optimal policy | Fastest convergence |

**Key finding**  
Value Iteration converges much faster while retaining optimality.

---

## ğŸ¯ Overall Insights

- Exploration needs **balance**, not pure greediness nor pure randomness  
- DP algorithms **guarantee** optimal solutions when the model is perfect  
- Value Iteration proves more **efficient** than Policy Iteration

---

## ğŸ‘¥ Team
Hao Chen Â· Simone de Vos Burchart  
Group 88

Score: **8.3 / 10**

Course: *Introduction to Reinforcement Learning â€“ Leiden University*
